{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intelligent User Interfaces: Assignment 1\n",
    "=========================================\n",
    "\n",
    "\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Named Entity Recognition showing tokenization, parts of speech tagging followed by named entity recognition for\n",
    "\n",
    "> Steve Jobs was an American entrepreneur and business magnate.\n",
    "> He was the chairman, chief executive officer (CEO), and a co-founder of Apple Inc.,\n",
    "> chairman and majority shareholder of Pixar, a member of The Walt Disney Company's\n",
    "> board of directors following its acquisition of Pixar, and the founder, chairman, and CEO of NeXT.\n",
    "> Jobs is widely recognized as a pioneer of the microcomputer\n",
    "> revolution of the 1970s and 1980s, along with Apple co-founder Steve Wozniak.\n",
    "\n",
    "\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "Extract all bigrams, trigrams using ngrams of nltk library\n",
    "\n",
    "> Machine learning is a necessary field in today's world.\n",
    "> Data science can do wonders\n",
    "> Natural Language Processing is how machines understand text\n",
    "\n",
    "\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "Sentiment analysis using Vader. Print polarity scores for each token along\n",
    "with compound scores for each sentence. Based on the compound scores,\n",
    "decide sentiment as positive (if >=0.05), negative (if<+0.05) or neutral otherwise.\n",
    "\n",
    "Sentences:\n",
    "\n",
    "- We are happy!\n",
    "- Today I am Happy\n",
    "- The best life ever\n",
    "- I am sad\n",
    "- We are sad\n",
    "- We are super sad\n",
    "- We are all so sad today\n",
    "\n",
    "\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Text Summarization of a Wikipedia article\n",
    "\n",
    "https://en.wikipedia.org/wiki/Artificial_intelligence\n",
    "\n",
    "1. Data collection from Wikipedia using web scraping(using Urllib library)\n",
    "2. Parsing the URL content of the data(using BeautifulSoup library)\n",
    "3. Data clean-up like removing special characters, numeric values, stop words and punctuations.\n",
    "4. Tokenization — Creation of tokens (Word tokens and Sentence tokens)\n",
    "5. Calculate the word frequency for each word.\n",
    "6. Calculate the weighted frequency for each sentence.\n",
    "7. Creation of summary choosing 30% of top weighted sentences.\n",
    "\n",
    "\n",
    "Problem 5\n",
    "---------\n",
    "\n",
    "Language detection Using NLTK Python and print the probabilities\n",
    "and language name for the following phrases:\n",
    "\n",
    "1. Solen skinner i dag, fuglene synger, og det er sommer.\n",
    "2. Ní dhéanfaidh ach Dia breithiúnas orm.\n",
    "3. I domum et cuna matrem tuam in cochleare.\n",
    "4. Huffa, huffa meg, det finns poteter på badet. Stakkars, stakkars meg, det finns poteter på badet.\n",
    "\n",
    "\n",
    "Problem 6\n",
    "---------\n",
    "\n",
    "Which problems do adaptive and predictive keyboards address?\n",
    "Explain how touch information and language information can be combined for keyboard adaptation.\n",
    "Explain decoding of touch sequences with token passing and beam pruning. Using Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
