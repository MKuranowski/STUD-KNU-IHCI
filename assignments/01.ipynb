{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intelligent User Interfaces: Assignment 1\n",
    "=========================================\n",
    "\n",
    "Course: Intelligent Human-Computer Interface (COMP0455)  \n",
    "Student: Mikolaj Kuranowski (2020427681)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import heapq\n",
    "import spacy\n",
    "import re\n",
    "import urllib.request\n",
    "import IPython.display\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from spacy import displacy\n",
    "from table2md import MarkdownTable\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 1\n",
    "---------\n",
    "\n",
    "Named Entity Recognition showing tokenization, parts of speech tagging followed by named entity recognition for\n",
    "\n",
    "> Steve Jobs was an American entrepreneur and business magnate.\n",
    "> He was the chairman, chief executive officer (CEO), and a co-founder of Apple Inc.,\n",
    "> chairman and majority shareholder of Pixar, a member of The Walt Disney Company's\n",
    "> board of directors following its acquisition of Pixar, and the founder, chairman, and CEO of NeXT.\n",
    "> Jobs is widely recognized as a pioneer of the microcomputer\n",
    "> revolution of the 1970s and 1980s, along with Apple co-founder Steve Wozniak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Steve Jobs was an American entrepreneur and business magnate.\n",
    "He was the chairman, chief executive officer (CEO), and a co-founder of Apple Inc.,\n",
    "chairman and majority shareholder of Pixar, a member of The Walt Disney Company's\n",
    "board of directors following its acquisition of Pixar, and the founder, chairman, and CEO of NeXT.\n",
    "Jobs is widely recognized as a pioneer of the microcomputer\n",
    "revolution of the 1970s and 1980s, along with Apple co-founder Steve Wozniak.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Steve',\n",
       " 'Jobs',\n",
       " 'was',\n",
       " 'an',\n",
       " 'American',\n",
       " 'entrepreneur',\n",
       " 'and',\n",
       " 'business',\n",
       " 'magnate',\n",
       " '.',\n",
       " 'He',\n",
       " 'was',\n",
       " 'the',\n",
       " 'chairman',\n",
       " ',']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(text)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Steve Jobs\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " was an \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    American\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " entrepreneur and business magnate.</br>He was the chairman, chief executive officer (CEO), and a co-founder of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple Inc.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ",</br>chairman and majority shareholder of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Pixar\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", a member of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    The Walt Disney Company's\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "</br>board of directors following its acquisition of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Pixar\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", and the founder, chairman, and CEO of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    NeXT\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ".</br>\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Jobs\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " is widely recognized as a pioneer of the microcomputer</br>revolution of the \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1970s\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1980s\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", along with \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " co-founder \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Steve Wozniak\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ".</br></div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(nlp(text), jupyter=True, style=\"ent\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2\n",
    "---------\n",
    "\n",
    "Extract all bigrams, trigrams using ngrams of nltk library\n",
    "\n",
    "> Machine learning is a necessary field in today's world.\n",
    "> Data science can do wonders\n",
    "> Natural Language Processing is how machines understand text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Machine learning is a necessary field in today's world.\n",
    "Data science can do wonders.\n",
    "Natural Language Processing is how machines understand text.\n",
    "\"\"\"\n",
    "tokens = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Machine', 'learning'),\n",
       " ('learning', 'is'),\n",
       " ('is', 'a'),\n",
       " ('a', 'necessary'),\n",
       " ('necessary', 'field'),\n",
       " ('field', 'in'),\n",
       " ('in', 'today'),\n",
       " ('today', \"'s\"),\n",
       " (\"'s\", 'world'),\n",
       " ('world', '.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.ngrams(tokens, 2))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Machine', 'learning', 'is'),\n",
       " ('learning', 'is', 'a'),\n",
       " ('is', 'a', 'necessary'),\n",
       " ('a', 'necessary', 'field'),\n",
       " ('necessary', 'field', 'in'),\n",
       " ('field', 'in', 'today'),\n",
       " ('in', 'today', \"'s\"),\n",
       " ('today', \"'s\", 'world'),\n",
       " (\"'s\", 'world', '.'),\n",
       " ('world', '.', 'Data')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.ngrams(tokens, 3))[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 3\n",
    "---------\n",
    "\n",
    "Sentiment analysis using Vader. Print polarity scores for each token along\n",
    "with compound scores for each sentence. Based on the compound scores,\n",
    "decide sentiment as positive (if >=0.05), negative (if<+0.05) or neutral otherwise.\n",
    "\n",
    "Sentences:\n",
    "\n",
    "- We are happy!\n",
    "- Today I am Happy\n",
    "- The best life ever\n",
    "- I am sad\n",
    "- We are sad\n",
    "- We are super sad\n",
    "- We are all so sad today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"We are happy!\",\n",
    "    \"Today I am Happy\",\n",
    "    \"The best life ever\",\n",
    "    \"I am sad\",\n",
    "    \"We are sad\",\n",
    "    \"We are super sad\",\n",
    "    \"We are all so sad today\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "|         Sentence        | Sentiment | Compound Score |\n",
       "|-------------------------|-----------|----------------|\n",
       "| We are happy!           | positive  | 0.6114         |\n",
       "| Today I am Happy        | positive  | 0.5719         |\n",
       "| The best life ever      | positive  | 0.6369         |\n",
       "| I am sad                | negative  | -0.4767        |\n",
       "| We are sad              | negative  | -0.4767        |\n",
       "| We are super sad        | positive  | 0.2023         |\n",
       "| We are all so sad today | negative  | -0.6113        |\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    compound_score = SentimentIntensityAnalyzer().polarity_scores(sentence)[\"compound\"]\n",
    "    \n",
    "    sentiment = \"neutral\"\n",
    "    if compound_score >= 0.05: sentiment = \"positive\"\n",
    "    if compound_score <= -0.05: sentiment = \"negative\"\n",
    "\n",
    "    data.append({\"Sentence\": sentence, \"Sentiment\": sentiment, \"Compound Score\": compound_score})\n",
    "\n",
    "MarkdownTable.from_dicts(data).display()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 4\n",
    "---------\n",
    "\n",
    "Text Summarization of a Wikipedia article\n",
    "\n",
    "https://en.wikipedia.org/wiki/Artificial_intelligence\n",
    "\n",
    "1. Data collection from Wikipedia using web scraping(using Urllib library)\n",
    "2. Parsing the URL content of the data(using BeautifulSoup library)\n",
    "3. Data clean-up like removing special characters, numeric values, stop words and punctuations.\n",
    "4. Tokenization — Creation of tokens (Word tokens and Sentence tokens)\n",
    "5. Calculate the word frequency for each word.\n",
    "6. Calculate the weighted frequency for each sentence.\n",
    "7. Creation of summary choosing 30% of top weighted sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Download the article\n",
    "with urllib.request.urlopen(\"https://en.wikipedia.org/wiki/Artificial_intelligence\") as f:\n",
    "    text = str(f.read(), \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Parse the HTML content and extract the main article\n",
    "paragraphs = BeautifulSoup(text, \"html.parser\").find(id=\"mw-content-text\").find_all(\"p\")\n",
    "article = \" \".join(p.text for p in paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Keep only words from the article and basic punctuation\n",
    "clean_article = re.sub(r\"\\[[0-9a-zA-Z]*\\]\", \"\", article)\n",
    "clean_article = re.sub(r\"\\s+\", \" \", clean_article)\n",
    "\n",
    "article_words = re.sub(r\"[^a-zA-Z]\", \" \", clean_article)\n",
    "article_words = re.sub(r\"\\s+\", \" \", article_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Tokenize the words\n",
    "sentences = nltk.sent_tokenize(clean_article)\n",
    "words = nltk.word_tokenize(article_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Calculate word frequency\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "tf = Counter(filter(lambda w: w not in stop_words, map(str.casefold, words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Score every sentence\n",
    "sentence_scores = {}\n",
    "for sentence in sentences:\n",
    "    sentence_scores[sentence] = 0\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "\n",
    "    if len(sentence_words) >= 30:\n",
    "        continue\n",
    "\n",
    "    for word in sentence_words:\n",
    "        word = word.casefold()\n",
    "        sentence_scores[sentence] += tf.get(word, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to intelligence of humans and other animals.\n",
       "AI founder John McCarthy agreed, writing that \"Artificial intelligence is not, by definition, simulation of human intelligence\".\n",
       "Much of current research involves statistical AI, which is overwhelmingly used to solve specific problems, even highly successful techniques such as deep learning.\n",
       "By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as \"artificial intelligence\".\n",
       "The next few years would later be called an \"AI winter\", a period when obtaining funding for AI projects was difficult.\n",
       "General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions.\n",
       "Many problems in AI (including in reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information.\n",
       "Frequently, when a technique reaches mainstream use, it is no longer considered artificial intelligence; this phenomenon is described as the AI effect.\n",
       "Hardware developed for AI includes AI accelerators and neuromorphic computing.\n",
       "The term artificial intelligence has also been criticized for overhyping AI's true technological capabilities.\n",
       "A machine with general intelligence can solve a wide variety of problems with breadth and versatility similar to human intelligence.\n",
       "Deep learning has drastically improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, image classification and others.\n",
       "There are also thousands of successful AI applications used to solve problems for specific industries or institutions.\n",
       "AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics.\n",
       "AI can solve many problems by intelligently searching through many possible solutions.\n",
       "Soft computing was introduced in the late 80s and most successful AI programs in the 21st century are examples of soft computing with neural networks.\n",
       "Symbolic AI (or \"GOFAI\") simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics.\n",
       "Controllers do, however, also classify conditions before inferring actions, and therefore classification forms a central part of many AI systems.\n",
       "Even humans rarely use the step-by-step deduction that early AI research could model.\n",
       "A superintelligence, hyperintelligence, or superhuman intelligence, is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind.\n",
       "The various sub-fields of AI research are centered around particular goals and the use of particular tools.\n",
       "Computer scientists and philosophers have since suggested that AI may become an existential risk to humanity if its rational capacities are not steered towards beneficial goals.\n",
       "A number of researchers began to look into \"sub-symbolic\" approaches to specific AI problems.\n",
       "AI also draws upon computer science, psychology, linguistics, philosophy, and many other fields.\n",
       "Knowledge representation and knowledge engineering allow AI programs to answer questions intelligently and make deductions about real-world facts.\n",
       "Symbolic AI used formal syntax to translate the deep structure of sentences into logic.\n",
       "Several different forms of logic are used in AI research.\n",
       "AI-related patents not only disclose AI techniques and applications, they often also refer to an application field or industry.\n",
       "The first work that is now generally recognized as AI was McCullouch and Pitts' 1943 formal design for Turing-complete \"artificial neurons\".\n",
       "Numerous academic researchers became concerned that AI was no longer pursuing the original goal of creating versatile, fully intelligent machines."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 7. Creation of summary choosing 30 of top weighted sentences.\n",
    "summary_sentences = heapq.nlargest(30, sentence_scores.keys(), key=sentence_scores.get)\n",
    "summary = \"\\n\".join(summary_sentences)\n",
    "\n",
    "IPython.display.display_pretty(summary, raw=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 5\n",
    "---------\n",
    "\n",
    "Language detection Using NLTK Python and print the probabilities\n",
    "and language name for the following phrases:\n",
    "\n",
    "1. Solen skinner i dag, fuglene synger, og det er sommer.\n",
    "2. Ní dhéanfaidh ach Dia breithiúnas orm.\n",
    "3. I domum et cuna matrem tuam in cochleare.\n",
    "4. Huffa, huffa meg, det finns poteter på badet. Stakkars, stakkars meg, det finns poteter på badet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "|                                              Sentence                                             | 1st most likely language (distance) | 2nd most likely language (distance) | 3rd most likely language (distance) |\n",
       "|---------------------------------------------------------------------------------------------------|-------------------------------------|-------------------------------------|-------------------------------------|\n",
       "| Solen skinner i dag, fuglene synger, og det er sommer.                                            | nob (17231.0)                       | nno (18845.0)                       | dan (24349.0)                       |\n",
       "| Ní dhéanfaidh ach Dia breithiúnas orm.                                                            | gle (15746.0)                       | sun (4.611686018427397e+19)         | eng (7.378697629483826e+19)         |\n",
       "| I domum et cuna matrem tuam in cochleare.                                                         | eng  (54494.0)                      | eng (57954.0)                       | fra (62543.0)                       |\n",
       "| Huffa, huffa meg, det finns poteter på badet. Stakkars, stakkars meg, det finns poteter på badet. | nno (9.223372036854817e+18)         | nob (9.223372036854823e+18)         | dan (1.8446744073709597e+19)        |\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Solution adapted from: https://www.nltk.org/_modules/nltk/classify/textcat.html\n",
    "# Note that the model doesn't use \"probability\", but \"distance\" from a language.\n",
    "#\n",
    "# Distance can be arbitrarily large, assume Inf. Therefore, probability makes no sense,\n",
    "# as (in floating point numbers, anyway) anything divided by Inf is zero.\n",
    "\n",
    "sentences = [\n",
    "    \"Solen skinner i dag, fuglene synger, og det er sommer.\",\n",
    "    \"Ní dhéanfaidh ach Dia breithiúnas orm.\",\n",
    "    \"I domum et cuna matrem tuam in cochleare.\",\n",
    "    \"Huffa, huffa meg, det finns poteter på badet. Stakkars, stakkars meg, det finns poteter på badet.\",\n",
    "]\n",
    "\n",
    "tc = nltk.TextCat()\n",
    "data = []\n",
    "for sentence in sentences:\n",
    "    distances = {k: float(v) for k, v in tc.lang_dists(sentence).items()}\n",
    "    most_likely = heapq.nsmallest(3, distances, key=distances.get)\n",
    "    data.append({\n",
    "        \"Sentence\": sentence,\n",
    "        \"1st most likely language (distance)\": f\"{most_likely[0]} ({distances[most_likely[0]]})\",\n",
    "        \"2nd most likely language (distance)\": f\"{most_likely[1]} ({distances[most_likely[1]]})\",\n",
    "        \"3rd most likely language (distance)\": f\"{most_likely[2]} ({distances[most_likely[2]]})\",\n",
    "    })\n",
    "MarkdownTable.from_dicts(data).display()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 6\n",
    "---------\n",
    "\n",
    "Which problems do adaptive and predictive keyboards address?\n",
    "Explain how touch information and language information can be combined for keyboard adaptation.\n",
    "Explain decoding of touch sequences with token passing and beam pruning. Using Algorithm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
